{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed61cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader,PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "import operator\n",
    "from langchain.docstore.document import Document\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc63908",
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_key=\"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\"\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=paid_key)\n",
    "\n",
    "import tweepy\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAACsy4QEAAAAAl%2F6hXVFuAw1ih2GBPjR%2BHJQkxZI%3DGeVeQJ2AZNU9HTbE1ajiwaVAvaUIrtCfRO9jE7hOEk1Ybb6Gj0\")\n",
    "\n",
    "# Step 1: Get user ID for the handle\n",
    "user = client.get_user(username=\"Cysic_xyz\")\n",
    "user_id = user.data.id\n",
    "\n",
    "# Step 2: Get tweets\n",
    "tweets = client.get_users_tweets(\n",
    "    id=user_id,\n",
    "    max_results=10,  # latest 10 tweets\n",
    "    tweet_fields=[\"created_at\", \"text\"]\n",
    ")\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(f\"[{tweet.created_at}] {tweet.text}\")\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# --- Local TXT Retriever ---\n",
    "doc_loader = TextLoader(r\"C:\\Users\\HP\\Desktop\\Twitter Thread Creator\\Cysic whitepaper.txt\")\n",
    "docs = doc_loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "docs_chunk = text_splitter.split_documents(docs)\n",
    "\n",
    "pdf_store = Chroma.from_documents(\n",
    "    documents=docs_chunk,\n",
    "    collection_name=\"text_docs\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_store_txt\"\n",
    ")\n",
    "txt_retriever = pdf_store.as_retriever(search_kwargs={\"k\": 5})  # Limit results\n",
    "\n",
    "# --- URL Retriever (lighter WebBaseLoader) ---\n",
    "base_urls = [\n",
    "    \"https://docs.cysic.xyz/readme/cysic-agent-to-agent-protocol\",\n",
    "    \"https://docs.cysic.xyz/\"\n",
    "]\n",
    "url_loader = WebBaseLoader(base_urls)\n",
    "url_docs = url_loader.load()\n",
    "url_chunks = text_splitter.split_documents(url_docs)\n",
    "\n",
    "url_store = Chroma.from_documents(\n",
    "    documents=url_chunks,\n",
    "    collection_name=\"url_docs\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_store_url\"\n",
    ")\n",
    "url_retriever = url_store.as_retriever(search_kwargs={\"k\": 5})  # Limit results\n",
    "\n",
    "# --- Tweet Retriever (Enhanced filtering for meaningful content) ---\n",
    "def filter_meaningful_tweets(tweets):\n",
    "    \"\"\"Filter and process tweets to store only meaningful, generic content\"\"\"\n",
    "    if not tweets:  # Handle empty list\n",
    "        return []\n",
    "        \n",
    "    meaningful_tweets = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # Ensure tweet is a string\n",
    "        if not isinstance(tweet, str):\n",
    "            continue\n",
    "            \n",
    "        # Skip if too short (less than 10 characters to be more lenient)\n",
    "        if len(tweet.strip()) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Skip if contains too many mentions or hashtags (spam-like)\n",
    "        mention_count = tweet.count('@')\n",
    "        hashtag_count = tweet.count('#')\n",
    "        if mention_count > 5 or hashtag_count > 8:  # More lenient\n",
    "            continue\n",
    "            \n",
    "        # Skip promotional/spam content\n",
    "        spam_keywords = ['buy now', 'limited time', 'click here', 'dm me', 'follow for follow', 'get rich quick']\n",
    "        if any(keyword in tweet.lower() for keyword in spam_keywords):\n",
    "            continue\n",
    "            \n",
    "        # Keep educational, informational, or engaging content\n",
    "        meaningful_keywords = ['learn', 'understand', 'explore', 'discover', 'innovation', \n",
    "                             'technology', 'development', 'community', 'future', 'breakthrough',\n",
    "                             'cysic', 'blockchain', 'crypto', 'defi', 'web3', 'protocol']\n",
    "        \n",
    "        # More lenient criteria: keep if has meaningful keywords OR is reasonably long OR mentions project\n",
    "        if (any(keyword in tweet.lower() for keyword in meaningful_keywords) or \n",
    "            len(tweet) > 80 or \n",
    "            'cysic' in tweet.lower()):\n",
    "            meaningful_tweets.append(tweet.strip())\n",
    "    \n",
    "    return meaningful_tweets\n",
    "\n",
    "# Process Tweepy response object properly\n",
    "def extract_tweet_texts(tweets_response):\n",
    "    \"\"\"Extract tweet texts from Tweepy response object\"\"\"\n",
    "    if not tweets_response or not hasattr(tweets_response, 'data') or not tweets_response.data:\n",
    "        return []\n",
    "    \n",
    "    tweet_texts = []\n",
    "    for tweet in tweets_response.data:\n",
    "        if hasattr(tweet, 'text'):\n",
    "            tweet_texts.append(tweet.text)\n",
    "    \n",
    "    return tweet_texts\n",
    "\n",
    "# Check if 'tweets' variable exists and handle appropriately\n",
    "try:\n",
    "    if 'tweets' in locals() or 'tweets' in globals():\n",
    "        # Extract text from Tweepy response\n",
    "        raw_tweet_texts = extract_tweet_texts(tweets)\n",
    "        filtered_tweets = filter_meaningful_tweets(raw_tweet_texts)\n",
    "        print(f\"Original tweets: {len(raw_tweet_texts)}\")\n",
    "        print(f\"Filtered tweets: {len(filtered_tweets)}\")\n",
    "        \n",
    "        # Print sample of filtered tweets for debugging\n",
    "        if filtered_tweets:\n",
    "            print(\"\\nSample filtered tweets:\")\n",
    "            for i, tweet in enumerate(filtered_tweets[:3], 1):\n",
    "                print(f\"{i}. {tweet[:100]}...\")\n",
    "                \n",
    "    elif 'tweet' in locals() or 'tweet' in globals():\n",
    "        # Handle if you named it 'tweet' instead of 'tweets'\n",
    "        if hasattr(tweet, 'data'):\n",
    "            raw_tweet_texts = extract_tweet_texts(tweet)\n",
    "            filtered_tweets = filter_meaningful_tweets(raw_tweet_texts)\n",
    "        else:\n",
    "            filtered_tweets = filter_meaningful_tweets(tweet)\n",
    "        print(f\"Filtered tweets: {len(filtered_tweets)}\")\n",
    "        \n",
    "    else:\n",
    "        # If no tweets available, create some sample meaningful tweets for testing\n",
    "        print(\"No 'tweets' variable found. Using sample tweets for testing.\")\n",
    "        sample_tweets = [\n",
    "            \"Exploring the future of decentralized protocols with Cysic's innovative approach to blockchain technology.\",\n",
    "            \"The community is growing! Excited to see more developers joining the Cysic ecosystem.\",\n",
    "            \"Understanding zero-knowledge proofs and their role in modern cryptocurrency systems.\",\n",
    "            \"Innovation in blockchain requires both technical excellence and community collaboration.\",\n",
    "            \"Building the future of Web3 infrastructure one protocol at a time.\"\n",
    "        ]\n",
    "        filtered_tweets = sample_tweets\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing tweets: {e}\")\n",
    "    # Fallback to sample tweets\n",
    "    filtered_tweets = [\n",
    "        \"Cysic is revolutionizing blockchain technology through innovative protocols.\",\n",
    "        \"Join our growing community of developers building the future of DeFi.\",\n",
    "        \"Learn about the latest developments in zero-knowledge proof systems.\"\n",
    "    ]\n",
    "\n",
    "# Only create tweet store if we have tweets\n",
    "if filtered_tweets:\n",
    "    tweet_docs = [Document(page_content=t, metadata={\"source\": \"twitter\", \"type\": \"meaningful_content\"}) \n",
    "                  for t in filtered_tweets]\n",
    "\n",
    "    tweet_store = Chroma.from_documents(\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"tweet_docs\",\n",
    "        documents=tweet_docs,\n",
    "        persist_directory=\"./chroma_store_tweet\"\n",
    "    )\n",
    "    tweet_retriever = tweet_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "else:\n",
    "    print(\"Warning: No meaningful tweets found. Tweet retriever will return empty results.\")\n",
    "    # Create a dummy retriever that returns empty results\n",
    "    class EmptyRetriever:\n",
    "        def get_relevant_documents(self, query):\n",
    "            return []\n",
    "    \n",
    "    tweet_retriever = EmptyRetriever()\n",
    "\n",
    "# -------------------------\n",
    "# TOOLS (Enhanced with better content processing)\n",
    "# -------------------------\n",
    "MAX_OUTPUT_CHARS = 4000  # Reduced for better LLM processing\n",
    "\n",
    "@tool\n",
    "def text_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant content from local TXT (Cysic whitepaper) and extract key themes.\"\"\"\n",
    "    results = txt_retriever.get_relevant_documents(query)\n",
    "    if not results:\n",
    "        return \"No relevant content found in whitepaper.\"\n",
    "    \n",
    "    content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "    return f\"WHITEPAPER CONTENT:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "\n",
    "@tool\n",
    "def url_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant content from Cysic documentation URLs.\"\"\"\n",
    "    results = url_retriever.get_relevant_documents(query)\n",
    "    if not results:\n",
    "        return \"No relevant content found in documentation.\"\n",
    "    \n",
    "    content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "    return f\"DOCUMENTATION CONTENT:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "\n",
    "@tool\n",
    "def tweet_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve meaningful tweet examples and recent updates.\"\"\"\n",
    "    try:\n",
    "        results = tweet_retriever.get_relevant_documents(query)\n",
    "        if not results:\n",
    "            return \"No relevant tweets found. Focus on creating original content based on whitepaper and documentation.\"\n",
    "        \n",
    "        content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "        return f\"RECENT MEANINGFUL TWEETS:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "    except Exception as e:\n",
    "        return \"Tweet retriever not available. Focus on creating original content from other sources.\"\n",
    "\n",
    "@tool\n",
    "def content_synthesizer(retrieved_content: str) -> str:\n",
    "    \"\"\"Synthesize content into detailed tweet themes with specific information.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing rich content from Cysic's documentation, whitepaper, and community discussions. \n",
    "    Extract the most compelling, specific, and detailed information to create substantial tweet content.\n",
    "\n",
    "    RETRIEVED CONTENT:\n",
    "\n",
    "    {retrieved_content}\n",
    "\n",
    "    Create 3-5 detailed tweet concepts that include:\n",
    "\n",
    "    1. **SPECIFIC TECHNICAL DETAILS**: Extract exact technical innovations, numbers, protocols, features\n",
    "    2. **CONCRETE EXAMPLES**: Real use cases, implementations, or applications mentioned\n",
    "    3. **UNIQUE VALUE PROPOSITIONS**: What makes Cysic different from competitors\n",
    "    4. **FACTUAL CLAIMS**: Specific benefits, performance metrics, or capabilities\n",
    "    5. **EDUCATIONAL INSIGHTS**: Deep explanations that teach something valuable\n",
    "\n",
    "\n",
    "    Focus on Kaito-approved content types:\n",
    "    - The core factual insight (with specific details from the content)\n",
    "    - Supporting technical explanation\n",
    "    - Real-world application or benefit\n",
    "    - Why this matters to the crypto/blockchain community\n",
    "    - Deep technical insights and explanations\n",
    "    - Market analysis and ecosystem developments  \n",
    "    - Educational content that teaches something new\n",
    "    - Contrarian takes backed by solid reasoning\n",
    "    - Future predictions with logical foundations\n",
    "    - Real-world use cases and adoption stories\n",
    "    - Create Quality, Original Content\n",
    "    - Use punchy, easy-to-read sentences that encourage engagement.\n",
    "    - Focus on authentic, insightful posts that provide unique value and deepen the crypto conversation. \n",
    "\n",
    "\n",
    "    DO NOT create generic themes. USE THE ACTUAL INFORMATION from the retrieved content.\n",
    "    Focus on substance over style - we want information-dense, valuable content.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "@tool\n",
    "def tweet_creator(content: str) -> str:\n",
    "        \"\"\"Create information-rich, substantial tweets from synthesized content.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Transform this detailed content into compelling, information-packed tweets.\n",
    "\n",
    "        SYNTHESIZED CONTENT:\n",
    "        {content}\n",
    "\n",
    "        Create 2-3 substantial tweets that are INFORMATION-DENSE and VALUE-PACKED:\n",
    "\n",
    "        REQUIREMENTS:\n",
    "        ✅ LEAD WITH SPECIFIC FACTS: Start with concrete technical details, not questions\n",
    "        ✅ INCLUDE EXACT INFORMATION: Use specific features, numbers, capabilities from the content\n",
    "        ✅ EXPLAIN HOW/WHY: Don't just state what, explain the mechanism or reasoning\n",
    "        ✅ PROVIDE CONTEXT: Why this matters, what problem it solves\n",
    "        ✅ USE THREAD FORMAT: Each tweet should build on the previous one\n",
    "        ✅ MAXIMIZE CHARACTER COUNT: Use close to 280 characters with valuable information\n",
    "        ✅ INCLUDE TECHNICAL DEPTH: Show genuine expertise and understanding\n",
    "\n",
    "        STRUCTURE EACH TWEET:\n",
    "        - Hook: Bold factual statement or counterintuitive insight\n",
    "        - Body: Detailed explanation with specifics\n",
    "        - Context: Why this matters or what it enables\n",
    "        - Optional: Implication or next step\n",
    "\n",
    "        AVOID:\n",
    "        ❌ Starting with questions\n",
    "        ❌ Generic statements without specifics\n",
    "        ❌ Vague claims without backing details\n",
    "        ❌ Filler words that waste character count\n",
    "        ❌ Asking \"What do you think?\" type endings\n",
    "\n",
    "        EXAMPLE GOOD START:\n",
    "        \"Cysic's Agent-to-Agent Protocol eliminates the 40% overhead traditional blockchains face with cross-chain transactions by implementing direct peer verification through...\"\n",
    "\n",
    "        EXAMPLE BAD START:\n",
    "        \"Have you ever wondered about blockchain scalability? 🤔\"\n",
    "\n",
    "        Make each tweet a masterclass in the topic - something people will bookmark and reference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return response.content if hasattr(response, 'content') else str(response)\n",
    "        except Exception as e:\n",
    "            return f\"Error creating tweets: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def quality_reviewer(tweet_content: str) -> str:\n",
    "    \"\"\"Review and enhance tweets to maximize information density and value.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Review these tweets and enhance them to be information powerhouses:\n",
    "\n",
    "    CURRENT TWEETS:\n",
    "    {tweet_content}\n",
    "\n",
    "    ENHANCEMENT CRITERIA:\n",
    "\n",
    "    📊 INFORMATION DENSITY:\n",
    "    - Are we utilizing every character for maximum value?\n",
    "    - Can we add more specific technical details?\n",
    "    - Are there concrete examples or numbers we can include?\n",
    "\n",
    "    🎯 VALUE PROPOSITION:\n",
    "    - Will readers learn something genuinely new and useful?\n",
    "    - Is the technical depth appropriate for crypto Twitter audience?\n",
    "    - Does each tweet provide actionable insights or understanding?\n",
    "\n",
    "    🔥 ENGAGEMENT OPTIMIZATION:\n",
    "    - Does it position readers as early/informed about important developments?\n",
    "    - Will people want to share this because it makes them look smart?\n",
    "    - Is there enough substance to spark intelligent discussion?\n",
    "\n",
    "    ⚡ TWITTER OPTIMIZATION:\n",
    "    - Use bullet points or numbers for complex information\n",
    "    - Include relevant technical terms that show expertise\n",
    "    - Structure for easy reading and shareability\n",
    "\n",
    "    ENHANCE EACH TWEET BY:\n",
    "    1. Adding more specific details from the source content\n",
    "    2. Including concrete examples or use cases\n",
    "    3. Providing clearer explanations of technical concepts\n",
    "    4. Maximizing character count with valuable information\n",
    "    5. Ensuring each tweet can stand alone as valuable content\n",
    "\n",
    "    Return the enhanced version that transforms casual readers into informed community members.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content if hasattr(response, 'content') else str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in quality review: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def kaito_compliance_checker(content: str) -> str:\n",
    "    \"\"\"Optimize content for maximum information value and Kaito platform approval.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Transform this content into information-rich, Kaito-optimized tweets:\n",
    "    CURRENT CONTENT:\n",
    "    {content}\n",
    "    KAITO PLATFORM OPTIMIZATION:\n",
    "    🎯 INFORMATION MAXIMIZATION:\n",
    "    - Pack each tweet with specific, actionable insights\n",
    "    - Include concrete technical details and explanations\n",
    "    - Reference exact features, protocols, or capabilities\n",
    "    - Provide educational value that readers can immediately use\n",
    "    🔥 CONTENT DEPTH REQUIREMENTS:\n",
    "    - Each tweet should teach something specific about blockchain/crypto\n",
    "    - Include \"how it works\" explanations, not just \"what it is\"\n",
    "    - Show the underlying mechanics or technology\n",
    "    - Explain the \"why\" behind technical decisions\n",
    "    📊 KAITO SUCCESS METRICS:\n",
    "    - High bookmark rate (reference-worthy content)\n",
    "    - Quote tweets with technical discussions\n",
    "    - Replies with follow-up questions showing genuine interest\n",
    "    - Profile visits from people wanting to learn more\n",
    "    ⚡ TECHNICAL AUTHORITY INDICATORS:\n",
    "    - Use precise technical terminology correctly\n",
    "    - Reference specific protocols, algorithms, or implementations\n",
    "    - Include performance metrics or comparative advantages\n",
    "    - Demonstrate deep understanding of the technology stack\n",
    "    🎨 FORMATTING FOR MAXIMUM IMPACT:\n",
    "    - Use thread format (1/3, 2/3, 3/3) for complex topics\n",
    "    - Structure with clear takeaways\n",
    "    - Include relevant emojis for visual breaks\n",
    "    - End with implications or future developments\n",
    "    TRANSFORMATION FOCUS:\n",
    "    - Convert vague statements into specific technical explanations\n",
    "    - Replace questions with authoritative insights\n",
    "    - Add concrete examples and use cases\n",
    "    - Include \"under the hood\" technical details\n",
    "    Return content that establishes you as a technical authority while being accessible to the crypto community.\n",
    "    Each tweet should be dense with valuable, shareable information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content if hasattr(response, 'content') else str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in Kaito optimization: {str(e)}\"\n",
    "@tool\n",
    "def final_formatter(content: str) -> str:\n",
    "    \"\"\"Format final tweet.\"\"\"\n",
    "    return content.strip()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# AGENTS (Enhanced with specific roles)\n",
    "# -------------------------\n",
    "\n",
    "def research_node(state):\n",
    "    \"\"\"Enhanced research phase with more comprehensive data gathering\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    query = messages[-1].content\n",
    "    \n",
    "    # Gather comprehensive information from all sources\n",
    "    logger.info(f\"Researching query: {query}\")\n",
    "    \n",
    "    txt_content = text_loader.invoke({\"query\": query})\n",
    "    url_content = url_loader.invoke({\"query\": query})\n",
    "    tweet_content = tweet_loader.invoke({\"query\": query})\n",
    "    \n",
    "    # Create detailed research summary with more context\n",
    "    research_summary = f\"\"\"\n",
    "    COMPREHENSIVE RESEARCH FINDINGS FOR QUERY: \"{query}\"\n",
    "    \n",
    "    =================== WHITEPAPER INSIGHTS ===================\n",
    "    {txt_content}\n",
    "    \n",
    "    =================== DOCUMENTATION DETAILS ===================\n",
    "    {url_content}\n",
    "    \n",
    "    =================== COMMUNITY CONTEXT ===================\n",
    "    {tweet_content}\n",
    "    \n",
    "    =================== RESEARCH SYNTHESIS ===================\n",
    "    The above content represents detailed technical information, documentation, and community insights about Cysic.\n",
    "    This should be used to create substantial, information-rich content that demonstrates deep technical understanding.\n",
    "    Focus on extracting specific features, technical innovations, use cases, and unique value propositions.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=research_summary)]}\n",
    "def synthesis_node(state):\n",
    "    \"\"\"Enhanced synthesis with focus on substantial content creation\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    research_data = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Synthesizing research into detailed themes\")\n",
    "    \n",
    "    themes = content_synthesizer.invoke({\"retrieved_content\": research_data})\n",
    "    \n",
    "    # Add instruction for the next phase\n",
    "    enhanced_themes = f\"\"\"\n",
    "    DETAILED CONTENT THEMES:\n",
    "    {themes}\n",
    "    \n",
    "    INSTRUCTION FOR TWEET CREATION:\n",
    "    Use the above detailed themes to create information-dense, educational tweets that showcase deep technical knowledge.\n",
    "    Each tweet should be packed with specific details, concrete examples, and valuable insights from the research.\n",
    "    Avoid generic statements and focus on unique technical aspects and real-world applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=enhanced_themes)]}\n",
    "def creation_node(state):\n",
    "    \"\"\"Enhanced creation with emphasis on substantial, informative content\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    themes_content = messages[-1].content\n",
    "    original_query = messages[0].content\n",
    "    \n",
    "    logger.info(\"Creating information-rich tweets\")\n",
    "    \n",
    "    # Enhanced creation input with clear instructions\n",
    "    creation_input = f\"\"\"\n",
    "    ORIGINAL USER REQUEST: {original_query}\n",
    "    \n",
    "    DETAILED RESEARCH AND THEMES:\n",
    "    {themes_content}\n",
    "    \n",
    "    CREATION GUIDELINES:\n",
    "    - Create substantial tweets that utilize the rich information from the research\n",
    "    - Each tweet should be information-dense and educational\n",
    "    - Include specific technical details, features, and explanations\n",
    "    - Use concrete examples and real-world applications\n",
    "    - Demonstrate deep understanding of Cysic's technology\n",
    "    - Make each tweet a valuable learning resource\n",
    "    - Avoid generic questions and focus on providing authoritative insights\n",
    "    \"\"\"\n",
    "    \n",
    "    created_tweets = tweet_creator.invoke({\"content\": creation_input})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=created_tweets)]}\n",
    "def quality_node(state):\n",
    "    \"\"\"Enhanced quality review with focus on maximizing information value\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    draft_tweet = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Reviewing and enhancing tweet quality\")\n",
    "    \n",
    "    polished_tweet = quality_reviewer.invoke({\"tweet_content\": draft_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=polished_tweet)]}\n",
    "def kaito_check_node(state):\n",
    "    \"\"\"Enhanced Kaito compliance with focus on technical authority\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    polished_tweet = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Optimizing for Kaito platform compliance\")\n",
    "    \n",
    "    kaito_optimized = kaito_compliance_checker.invoke({\"content\": polished_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=kaito_optimized)]}\n",
    "def final_node(state):\n",
    "    \"\"\"Final formatting and preparation for publication\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    kaito_tweet = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Finalizing tweet content\")\n",
    "    \n",
    "    final_result = final_formatter.invoke({\"content\": kaito_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=final_result)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.add_node(\"synthesis\", synthesis_node)\n",
    "workflow.add_node(\"creation\", creation_node)\n",
    "workflow.add_node(\"quality\", quality_node)\n",
    "workflow.add_node(\"kaito_check\", kaito_check_node)\n",
    "workflow.add_node(\"final\", final_node)\n",
    "\n",
    "workflow.set_entry_point(\"research\")\n",
    "workflow.add_edge(\"research\", \"synthesis\")\n",
    "workflow.add_edge(\"synthesis\", \"creation\")\n",
    "workflow.add_edge(\"creation\", \"quality\")\n",
    "workflow.add_edge(\"quality\", \"kaito_check\")\n",
    "workflow.add_edge(\"kaito_check\", \"final\")\n",
    "\n",
    "workflow.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
