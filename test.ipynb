{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader,PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "import operator\n",
    "from langchain.docstore.document import Document\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_key= os.getenv(\"paid_key\")\n",
    "bearer_token=os.getenv(\"bearer_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=os.getenv(paid_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bc1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-22 14:00:59+00:00] https://t.co/6c1rFx9j1r\n",
      "[2025-09-21 23:54:37+00:00] @berso444 @SuccinctLabs Gprove Gmsor\n",
      "[2025-09-21 22:23:24+00:00] @willssmith8 yezzir\n",
      "[2025-09-21 22:23:13+00:00] @IamPurkov @Vadim1590025 CYSOR STRONG 💚\n",
      "[2025-09-21 20:19:04+00:00] @nse2_nso2q @RaeWrh @AlexCooperWG We look good here\n",
      "[2025-09-21 11:37:52+00:00] RT @cysic_xyz: He missed the Cysic testnet &amp; the Compute Cube sale.\n",
      "\n",
      "But that’s not you, right? https://t.co/ZI8AXrPsb3\n",
      "[2025-09-21 01:51:52+00:00] @NexusLabs Oh heyyyy\n",
      "[2025-09-21 01:50:47+00:00] @remo_226 Yes yes yes\n",
      "[2025-09-21 01:49:57+00:00] @chroneEZ HELL YEAHH\n",
      "[2025-09-21 01:48:40+00:00] @hangjin162536 @munchanghw98379 https://t.co/JZFoKLyNs1\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# Step 1: Get user ID for the handle\n",
    "user = client.get_user(username=\"Cysic_xyz\")\n",
    "user_id = user.data.id\n",
    "\n",
    "# Step 2: Get tweets\n",
    "tweets = client.get_users_tweets(\n",
    "    id=user_id,\n",
    "    max_results=10,  # latest 10 tweets\n",
    "    tweet_fields=[\"created_at\", \"text\"]\n",
    ")\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(f\"[{tweet.created_at}] {tweet.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3057350",
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nToo Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTooManyRequests\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m user_id = user.data.id\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Step 2: Get tweets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m tweets = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_users_tweets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# latest 10 tweets\u001b[39;49;00m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtweet_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreated_at\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets.data:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweet.created_at\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweet.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Twitter Thread Creator\\venv\\Lib\\site-packages\\tweepy\\client.py:1593\u001b[39m, in \u001b[36mClient.get_users_tweets\u001b[39m\u001b[34m(self, id, user_auth, **params)\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_users_tweets\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m, *, user_auth=\u001b[38;5;28;01mFalse\u001b[39;00m, **params):\n\u001b[32m   1494\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"get_users_tweets( \\\u001b[39;00m\n\u001b[32m   1495\u001b[39m \u001b[33;03m        id, *, end_time=None, exclude=None, expansions=None, \\\u001b[39;00m\n\u001b[32m   1496\u001b[39m \u001b[33;03m        max_results=None, media_fields=None, pagination_token=None, \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1591\u001b[39m \u001b[33;03m    .. _here: https://developer.twitter.com/en/docs/twitter-ids\u001b[39;00m\n\u001b[32m   1592\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1593\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/2/users/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/tweets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpansions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedia.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpagination_token\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplace.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpoll.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msince_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtweet.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muntil_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1600\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_auth\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Twitter Thread Creator\\venv\\Lib\\site-packages\\tweepy\\client.py:129\u001b[39m, in \u001b[36mBaseClient._make_request\u001b[39m\u001b[34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m, method, route, params={}, endpoint_parameters=(), json=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m     data_type=\u001b[38;5;28;01mNone\u001b[39;00m, user_auth=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    126\u001b[39m ):\n\u001b[32m    127\u001b[39m     request_params = \u001b[38;5;28mself\u001b[39m._process_params(params, endpoint_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_type \u001b[38;5;129;01mis\u001b[39;00m requests.Response:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Twitter Thread Creator\\venv\\Lib\\site-packages\\tweepy\\client.py:115\u001b[39m, in \u001b[36mBaseClient.request\u001b[39m\u001b[34m(self, method, route, params, json, user_auth)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(method, route, params, json, user_auth)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m500\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterServerError(response)\n",
      "\u001b[31mTooManyRequests\u001b[39m: 429 Too Many Requests\nToo Many Requests"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader,PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "import operator\n",
    "from langchain.docstore.document import Document\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import logging\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=paid_key)\n",
    "\n",
    "import tweepy\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# Step 1: Get user ID for the handle\n",
    "user = client.get_user(username=\"Cysic_xyz\")\n",
    "user_id = user.data.id\n",
    "\n",
    "# Step 2: Get tweets\n",
    "tweets = client.get_users_tweets(\n",
    "    id=user_id,\n",
    "    max_results=10,  # latest 10 tweets\n",
    "    tweet_fields=[\"created_at\", \"text\"]\n",
    ")\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(f\"[{tweet.created_at}] {tweet.text}\")\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# --- Local TXT Retriever ---\n",
    "doc_loader = TextLoader(r\"C:\\Users\\HP\\Desktop\\Twitter Thread Creator\\Cysic whitepaper.txt\")\n",
    "docs = doc_loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "docs_chunk = text_splitter.split_documents(docs)\n",
    "\n",
    "pdf_store = Chroma.from_documents(\n",
    "    documents=docs_chunk,\n",
    "    collection_name=\"text_docs\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_store_txt\"\n",
    ")\n",
    "txt_retriever = pdf_store.as_retriever(search_kwargs={\"k\": 5})  # Limit results\n",
    "\n",
    "# --- URL Retriever (lighter WebBaseLoader) ---\n",
    "base_urls = [\n",
    "    \"https://docs.cysic.xyz/readme/cysic-agent-to-agent-protocol\",\n",
    "    \"https://docs.cysic.xyz/\"\n",
    "]\n",
    "url_loader = WebBaseLoader(base_urls)\n",
    "url_docs = url_loader.load()\n",
    "url_chunks = text_splitter.split_documents(url_docs)\n",
    "\n",
    "url_store = Chroma.from_documents(\n",
    "    documents=url_chunks,\n",
    "    collection_name=\"url_docs\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_store_url\"\n",
    ")\n",
    "url_retriever = url_store.as_retriever(search_kwargs={\"k\": 5})  # Limit results\n",
    "\n",
    "# --- Tweet Retriever (Enhanced filtering for meaningful content) ---\n",
    "def filter_meaningful_tweets(tweets):\n",
    "    \"\"\"Filter and process tweets to store only meaningful, generic content\"\"\"\n",
    "    if not tweets:  # Handle empty list\n",
    "        return []\n",
    "        \n",
    "    meaningful_tweets = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # Ensure tweet is a string\n",
    "        if not isinstance(tweet, str):\n",
    "            continue\n",
    "            \n",
    "        # Skip if too short (less than 10 characters to be more lenient)\n",
    "        if len(tweet.strip()) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Skip if contains too many mentions or hashtags (spam-like)\n",
    "        mention_count = tweet.count('@')\n",
    "        hashtag_count = tweet.count('#')\n",
    "        if mention_count > 5 or hashtag_count > 8:  # More lenient\n",
    "            continue\n",
    "            \n",
    "        # Skip promotional/spam content\n",
    "        spam_keywords = ['buy now', 'limited time', 'click here', 'dm me', 'follow for follow', 'get rich quick']\n",
    "        if any(keyword in tweet.lower() for keyword in spam_keywords):\n",
    "            continue\n",
    "            \n",
    "        # Keep educational, informational, or engaging content\n",
    "        meaningful_keywords = ['learn', 'understand', 'explore', 'discover', 'innovation', \n",
    "                             'technology', 'development', 'community', 'future', 'breakthrough',\n",
    "                             'cysic', 'blockchain', 'crypto', 'defi', 'web3', 'protocol']\n",
    "        \n",
    "        # More lenient criteria: keep if has meaningful keywords OR is reasonably long OR mentions project\n",
    "        if (any(keyword in tweet.lower() for keyword in meaningful_keywords) or \n",
    "            len(tweet) > 80 or \n",
    "            'cysic' in tweet.lower()):\n",
    "            meaningful_tweets.append(tweet.strip())\n",
    "    \n",
    "    return meaningful_tweets\n",
    "\n",
    "# Process Tweepy response object properly\n",
    "def extract_tweet_texts(tweets_response):\n",
    "    \"\"\"Extract tweet texts from Tweepy response object\"\"\"\n",
    "    if not tweets_response or not hasattr(tweets_response, 'data') or not tweets_response.data:\n",
    "        return []\n",
    "    \n",
    "    tweet_texts = []\n",
    "    for tweet in tweets_response.data:\n",
    "        if hasattr(tweet, 'text'):\n",
    "            tweet_texts.append(tweet.text)\n",
    "    \n",
    "    return tweet_texts\n",
    "\n",
    "# Check if 'tweets' variable exists and handle appropriately\n",
    "try:\n",
    "    if 'tweets' in locals() or 'tweets' in globals():\n",
    "        # Extract text from Tweepy response\n",
    "        raw_tweet_texts = extract_tweet_texts(tweets)\n",
    "        filtered_tweets = filter_meaningful_tweets(raw_tweet_texts)\n",
    "        print(f\"Original tweets: {len(raw_tweet_texts)}\")\n",
    "        print(f\"Filtered tweets: {len(filtered_tweets)}\")\n",
    "        \n",
    "        # Print sample of filtered tweets for debugging\n",
    "        if filtered_tweets:\n",
    "            print(\"\\nSample filtered tweets:\")\n",
    "            for i, tweet in enumerate(filtered_tweets[:3], 1):\n",
    "                print(f\"{i}. {tweet[:100]}...\")\n",
    "                \n",
    "    elif 'tweet' in locals() or 'tweet' in globals():\n",
    "        # Handle if you named it 'tweet' instead of 'tweets'\n",
    "        if hasattr(tweet, 'data'):\n",
    "            raw_tweet_texts = extract_tweet_texts(tweet)\n",
    "            filtered_tweets = filter_meaningful_tweets(raw_tweet_texts)\n",
    "        else:\n",
    "            filtered_tweets = filter_meaningful_tweets(tweet)\n",
    "        print(f\"Filtered tweets: {len(filtered_tweets)}\")\n",
    "        \n",
    "    else:\n",
    "        # If no tweets available, create some sample meaningful tweets for testing\n",
    "        print(\"No 'tweets' variable found. Using sample tweets for testing.\")\n",
    "        sample_tweets = [\n",
    "            \"Exploring the future of decentralized protocols with Cysic's innovative approach to blockchain technology.\",\n",
    "            \"The community is growing! Excited to see more developers joining the Cysic ecosystem.\",\n",
    "            \"Understanding zero-knowledge proofs and their role in modern cryptocurrency systems.\",\n",
    "            \"Innovation in blockchain requires both technical excellence and community collaboration.\",\n",
    "            \"Building the future of Web3 infrastructure one protocol at a time.\"\n",
    "        ]\n",
    "        filtered_tweets = sample_tweets\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing tweets: {e}\")\n",
    "    # Fallback to sample tweets\n",
    "    filtered_tweets = [\n",
    "        \"Cysic is revolutionizing blockchain technology through innovative protocols.\",\n",
    "        \"Join our growing community of developers building the future of DeFi.\",\n",
    "        \"Learn about the latest developments in zero-knowledge proof systems.\"\n",
    "    ]\n",
    "\n",
    "# Only create tweet store if we have tweets\n",
    "if filtered_tweets:\n",
    "    tweet_docs = [Document(page_content=t, metadata={\"source\": \"twitter\", \"type\": \"meaningful_content\"}) \n",
    "                  for t in filtered_tweets]\n",
    "\n",
    "    tweet_store = Chroma.from_documents(\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"tweet_docs\",\n",
    "        documents=tweet_docs,\n",
    "        persist_directory=\"./chroma_store_tweet\"\n",
    "    )\n",
    "    tweet_retriever = tweet_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "else:\n",
    "    print(\"Warning: No meaningful tweets found. Tweet retriever will return empty results.\")\n",
    "    # Create a dummy retriever that returns empty results\n",
    "    class EmptyRetriever:\n",
    "        def get_relevant_documents(self, query):\n",
    "            return []\n",
    "    \n",
    "    tweet_retriever = EmptyRetriever()\n",
    "\n",
    "# -------------------------\n",
    "# TOOLS (Enhanced with better content processing)\n",
    "# -------------------------\n",
    "MAX_OUTPUT_CHARS = 4000  # Reduced for better LLM processing\n",
    "\n",
    "@tool\n",
    "def text_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant content from local TXT (Cysic whitepaper) and extract key themes.\"\"\"\n",
    "    results = txt_retriever.get_relevant_documents(query)\n",
    "    if not results:\n",
    "        return \"No relevant content found in whitepaper.\"\n",
    "    \n",
    "    content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "    return f\"WHITEPAPER CONTENT:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "\n",
    "@tool\n",
    "def url_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant content from Cysic documentation URLs.\"\"\"\n",
    "    results = url_retriever.get_relevant_documents(query)\n",
    "    if not results:\n",
    "        return \"No relevant content found in documentation.\"\n",
    "    \n",
    "    content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "    return f\"DOCUMENTATION CONTENT:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "\n",
    "@tool\n",
    "def tweet_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve meaningful tweet examples and recent updates.\"\"\"\n",
    "    try:\n",
    "        results = tweet_retriever.get_relevant_documents(query)\n",
    "        if not results:\n",
    "            return \"No relevant tweets found. Focus on creating original content based on whitepaper and documentation.\"\n",
    "        \n",
    "        content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "        return f\"RECENT MEANINGFUL TWEETS:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "    except Exception as e:\n",
    "        return \"Tweet retriever not available. Focus on creating original content from other sources.\"\n",
    "\n",
    "@tool\n",
    "def content_synthesizer(retrieved_content: str) -> str:\n",
    "    \"\"\"Synthesize content into detailed tweet themes with specific information.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing rich content from Cysic's documentation, whitepaper, and community discussions. \n",
    "    Extract the most compelling, specific, and detailed information to create substantial tweet content.\n",
    "\n",
    "    RETRIEVED CONTENT:\n",
    "\n",
    "    {retrieved_content}\n",
    "\n",
    "    Create 3-5 detailed tweet concepts that include:\n",
    "\n",
    "    1. **SPECIFIC TECHNICAL DETAILS**: Extract exact technical innovations, numbers, protocols, features\n",
    "    2. **CONCRETE EXAMPLES**: Real use cases, implementations, or applications mentioned\n",
    "    3. **UNIQUE VALUE PROPOSITIONS**: What makes Cysic different from competitors\n",
    "    4. **FACTUAL CLAIMS**: Specific benefits, performance metrics, or capabilities\n",
    "    5. **EDUCATIONAL INSIGHTS**: Deep explanations that teach something valuable\n",
    "\n",
    "\n",
    "    Focus on Kaito-approved content types:\n",
    "    - The core factual insight (with specific details from the content)\n",
    "    - Supporting technical explanation\n",
    "    - Real-world application or benefit\n",
    "    - Why this matters to the crypto/blockchain community\n",
    "    - Deep technical insights and explanations\n",
    "    - Market analysis and ecosystem developments  \n",
    "    - Educational content that teaches something new\n",
    "    - Contrarian takes backed by solid reasoning\n",
    "    - Future predictions with logical foundations\n",
    "    - Real-world use cases and adoption stories\n",
    "    - Create Quality, Original Content\n",
    "    - Use punchy, easy-to-read sentences that encourage engagement.\n",
    "    - Focus on authentic, insightful posts that provide unique value and deepen the crypto conversation. \n",
    "\n",
    "\n",
    "    DO NOT create generic themes. USE THE ACTUAL INFORMATION from the retrieved content.\n",
    "    Focus on substance over style - we want information-dense, valuable content.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "@tool\n",
    "def tweet_creator(content: str) -> str:\n",
    "        \"\"\"Create information-rich, substantial tweets from synthesized content.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Transform this detailed content into compelling, information-packed tweets.\n",
    "\n",
    "        SYNTHESIZED CONTENT:\n",
    "        {content}\n",
    "\n",
    "        Create 2-3 substantial tweets that are INFORMATION-DENSE and VALUE-PACKED:\n",
    "\n",
    "        REQUIREMENTS:\n",
    "        ✅ LEAD WITH SPECIFIC FACTS: Start with concrete technical details, not questions\n",
    "        ✅ INCLUDE EXACT INFORMATION: Use specific features, numbers, capabilities from the content\n",
    "        ✅ EXPLAIN HOW/WHY: Don't just state what, explain the mechanism or reasoning\n",
    "        ✅ PROVIDE CONTEXT: Why this matters, what problem it solves\n",
    "        ✅ USE THREAD FORMAT: Each tweet should build on the previous one\n",
    "        ✅ MAXIMIZE CHARACTER COUNT: Use close to 280 characters with valuable information\n",
    "        ✅ INCLUDE TECHNICAL DEPTH: Show genuine expertise and understanding\n",
    "\n",
    "        STRUCTURE EACH TWEET:\n",
    "        - Hook: Bold factual statement or counterintuitive insight\n",
    "        - Body: Detailed explanation with specifics\n",
    "        - Context: Why this matters or what it enables\n",
    "        - Optional: Implication or next step\n",
    "\n",
    "        AVOID:\n",
    "        ❌ Starting with questions\n",
    "        ❌ Generic statements without specifics\n",
    "        ❌ Vague claims without backing details\n",
    "        ❌ Filler words that waste character count\n",
    "        ❌ Asking \"What do you think?\" type endings\n",
    "\n",
    "        EXAMPLE GOOD START:\n",
    "        \"Cysic's Agent-to-Agent Protocol eliminates the 40% overhead traditional blockchains face with cross-chain transactions by implementing direct peer verification through...\"\n",
    "\n",
    "        EXAMPLE BAD START:\n",
    "        \"Have you ever wondered about blockchain scalability? 🤔\"\n",
    "\n",
    "        Make each tweet a masterclass in the topic - something people will bookmark and reference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return response.content if hasattr(response, 'content') else str(response)\n",
    "        except Exception as e:\n",
    "            return f\"Error creating tweets: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def quality_reviewer(tweet_content: str) -> str:\n",
    "    \"\"\"Review and enhance tweets to maximize information density and value.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Review these tweets and enhance them to be information powerhouses:\n",
    "\n",
    "    CURRENT TWEETS:\n",
    "    {tweet_content}\n",
    "\n",
    "    ENHANCEMENT CRITERIA:\n",
    "\n",
    "    📊 INFORMATION DENSITY:\n",
    "    - Are we utilizing every character for maximum value?\n",
    "    - Can we add more specific technical details?\n",
    "    - Are there concrete examples or numbers we can include?\n",
    "\n",
    "    🎯 VALUE PROPOSITION:\n",
    "    - Will readers learn something genuinely new and useful?\n",
    "    - Is the technical depth appropriate for crypto Twitter audience?\n",
    "    - Does each tweet provide actionable insights or understanding?\n",
    "\n",
    "    🔥 ENGAGEMENT OPTIMIZATION:\n",
    "    - Does it position readers as early/informed about important developments?\n",
    "    - Will people want to share this because it makes them look smart?\n",
    "    - Is there enough substance to spark intelligent discussion?\n",
    "\n",
    "    ⚡ TWITTER OPTIMIZATION:\n",
    "    - Use bullet points or numbers for complex information\n",
    "    - Include relevant technical terms that show expertise\n",
    "    - Structure for easy reading and shareability\n",
    "\n",
    "    ENHANCE EACH TWEET BY:\n",
    "    1. Adding more specific details from the source content\n",
    "    2. Including concrete examples or use cases\n",
    "    3. Providing clearer explanations of technical concepts\n",
    "    4. Maximizing character count with valuable information\n",
    "    5. Ensuring each tweet can stand alone as valuable content\n",
    "\n",
    "    Return the enhanced version that transforms casual readers into informed community members.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content if hasattr(response, 'content') else str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in quality review: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def kaito_compliance_checker(content: str) -> str:\n",
    "    \"\"\"Optimize content for maximum information value and Kaito platform approval.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Transform this content into information-rich, Kaito-optimized tweets:\n",
    "    CURRENT CONTENT:\n",
    "    {content}\n",
    "    KAITO PLATFORM OPTIMIZATION:\n",
    "    🎯 INFORMATION MAXIMIZATION:\n",
    "    - Pack each tweet with specific, actionable insights\n",
    "    - Include concrete technical details and explanations\n",
    "    - Reference exact features, protocols, or capabilities\n",
    "    - Provide educational value that readers can immediately use\n",
    "    🔥 CONTENT DEPTH REQUIREMENTS:\n",
    "    - Each tweet should teach something specific about blockchain/crypto\n",
    "    - Include \"how it works\" explanations, not just \"what it is\"\n",
    "    - Show the underlying mechanics or technology\n",
    "    - Explain the \"why\" behind technical decisions\n",
    "    📊 KAITO SUCCESS METRICS:\n",
    "    - High bookmark rate (reference-worthy content)\n",
    "    - Quote tweets with technical discussions\n",
    "    - Replies with follow-up questions showing genuine interest\n",
    "    - Profile visits from people wanting to learn more\n",
    "    ⚡ TECHNICAL AUTHORITY INDICATORS:\n",
    "    - Use precise technical terminology correctly\n",
    "    - Reference specific protocols, algorithms, or implementations\n",
    "    - Include performance metrics or comparative advantages\n",
    "    - Demonstrate deep understanding of the technology stack\n",
    "    🎨 FORMATTING FOR MAXIMUM IMPACT:\n",
    "    - Use thread format (1/3, 2/3, 3/3) for complex topics\n",
    "    - Structure with clear takeaways\n",
    "    - Include relevant emojis for visual breaks\n",
    "    - End with implications or future developments\n",
    "    TRANSFORMATION FOCUS:\n",
    "    - Convert vague statements into specific technical explanations\n",
    "    - Replace questions with authoritative insights\n",
    "    - Add concrete examples and use cases\n",
    "    - Include \"under the hood\" technical details\n",
    "    Return content that establishes you as a technical authority while being accessible to the crypto community.\n",
    "    Each tweet should be dense with valuable, shareable information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content if hasattr(response, 'content') else str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in Kaito optimization: {str(e)}\"\n",
    "@tool\n",
    "def final_formatter(content: str) -> str:\n",
    "    \"\"\"Format final tweet.\"\"\"\n",
    "    return content.strip()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# AGENTS (Enhanced with specific roles)\n",
    "# -------------------------\n",
    "\n",
    "def research_node(state):\n",
    "    \"\"\"Enhanced research phase with more comprehensive data gathering\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    query = messages[-1].content\n",
    "    \n",
    "    # Gather comprehensive information from all sources\n",
    "    logger.info(f\"Researching query: {query}\")\n",
    "    \n",
    "    txt_content = text_loader.invoke({\"query\": query})\n",
    "    url_content = url_loader.invoke({\"query\": query})\n",
    "    tweet_content = tweet_loader.invoke({\"query\": query})\n",
    "    \n",
    "    # Create detailed research summary with more context\n",
    "    research_summary = f\"\"\"\n",
    "    COMPREHENSIVE RESEARCH FINDINGS FOR QUERY: \"{query}\"\n",
    "    \n",
    "    =================== WHITEPAPER INSIGHTS ===================\n",
    "    {txt_content}\n",
    "    \n",
    "    =================== DOCUMENTATION DETAILS ===================\n",
    "    {url_content}\n",
    "    \n",
    "    =================== COMMUNITY CONTEXT ===================\n",
    "    {tweet_content}\n",
    "    \n",
    "    =================== RESEARCH SYNTHESIS ===================\n",
    "    The above content represents detailed technical information, documentation, and community insights about Cysic.\n",
    "    This should be used to create substantial, information-rich content that demonstrates deep technical understanding.\n",
    "    Focus on extracting specific features, technical innovations, use cases, and unique value propositions.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=research_summary)]}\n",
    "def synthesis_node(state):\n",
    "    \"\"\"Enhanced synthesis with focus on substantial content creation\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    research_data = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Synthesizing research into detailed themes\")\n",
    "    \n",
    "    themes = content_synthesizer.invoke({\"retrieved_content\": research_data})\n",
    "    \n",
    "    # Add instruction for the next phase\n",
    "    enhanced_themes = f\"\"\"\n",
    "    DETAILED CONTENT THEMES:\n",
    "    {themes}\n",
    "    \n",
    "    INSTRUCTION FOR TWEET CREATION:\n",
    "    Use the above detailed themes to create information-dense, educational tweets that showcase deep technical knowledge.\n",
    "    Each tweet should be packed with specific details, concrete examples, and valuable insights from the research.\n",
    "    Avoid generic statements and focus on unique technical aspects and real-world applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=enhanced_themes)]}\n",
    "def creation_node(state):\n",
    "    \"\"\"Enhanced creation with emphasis on substantial, informative content\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    themes_content = messages[-1].content\n",
    "    original_query = messages[0].content\n",
    "    \n",
    "    logger.info(\"Creating information-rich tweets\")\n",
    "    \n",
    "    # Enhanced creation input with clear instructions\n",
    "    creation_input = f\"\"\"\n",
    "    ORIGINAL USER REQUEST: {original_query}\n",
    "    \n",
    "    DETAILED RESEARCH AND THEMES:\n",
    "    {themes_content}\n",
    "    \n",
    "    CREATION GUIDELINES:\n",
    "    - Create substantial tweets that utilize the rich information from the research\n",
    "    - Each tweet should be information-dense and educational\n",
    "    - Include specific technical details, features, and explanations\n",
    "    - Use concrete examples and real-world applications\n",
    "    - Demonstrate deep understanding of Cysic's technology\n",
    "    - Make each tweet a valuable learning resource\n",
    "    - Avoid generic questions and focus on providing authoritative insights\n",
    "    \"\"\"\n",
    "    \n",
    "    created_tweets = tweet_creator.invoke({\"content\": creation_input})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=created_tweets)]}\n",
    "def quality_node(state):\n",
    "    \"\"\"Enhanced quality review with focus on maximizing information value\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    draft_tweet = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Reviewing and enhancing tweet quality\")\n",
    "    \n",
    "    polished_tweet = quality_reviewer.invoke({\"tweet_content\": draft_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=polished_tweet)]}\n",
    "def kaito_check_node(state):\n",
    "    \"\"\"Enhanced Kaito compliance with focus on technical authority\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    polished_tweet = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Optimizing for Kaito platform compliance\")\n",
    "    \n",
    "    kaito_optimized = kaito_compliance_checker.invoke({\"content\": polished_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=kaito_optimized)]}\n",
    "def final_node(state):\n",
    "    \"\"\"Final formatting and preparation for publication\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    kaito_tweet = messages[-1].content\n",
    "    \n",
    "    logger.info(\"Finalizing tweet content\")\n",
    "    \n",
    "    final_result = final_formatter.invoke({\"content\": kaito_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=final_result)]}\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.add_node(\"synthesis\", synthesis_node)\n",
    "workflow.add_node(\"creation\", creation_node)\n",
    "workflow.add_node(\"quality\", quality_node)\n",
    "workflow.add_node(\"kaito_check\", kaito_check_node)\n",
    "workflow.add_node(\"final\", final_node)\n",
    "\n",
    "workflow.set_entry_point(\"research\")\n",
    "workflow.add_edge(\"research\", \"synthesis\")\n",
    "workflow.add_edge(\"synthesis\", \"creation\")\n",
    "workflow.add_edge(\"creation\", \"quality\")\n",
    "workflow.add_edge(\"quality\", \"kaito_check\")\n",
    "workflow.add_edge(\"kaito_check\", \"final\")\n",
    "\n",
    "workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afab703d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Embeddings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m embeddings = \u001b[43mHuggingFaceEmbeddings\u001b[49m(model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- Local TXT Retriever ---\u001b[39;00m\n\u001b[32m      5\u001b[39m doc_loader = TextLoader(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mHP\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mTwitter Thread Creator\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCysic whitepaper.txt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'HuggingFaceEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# --- Local TXT Retriever ---\n",
    "doc_loader = TextLoader(r\"C:\\Users\\HP\\Desktop\\Twitter Thread Creator\\Cysic whitepaper.txt\")\n",
    "docs = doc_loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "docs_chunk = text_splitter.split_documents(docs)\n",
    "\n",
    "pdf_store = Chroma.from_documents(\n",
    "    documents=docs_chunk,\n",
    "    collection_name=\"text_docs\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_store_txt\"\n",
    ")\n",
    "txt_retriever = pdf_store.as_retriever(search_kwargs={\"k\": 5})  # Limit results\n",
    "\n",
    "# --- URL Retriever (lighter WebBaseLoader) ---\n",
    "base_urls = [\n",
    "    \"https://docs.cysic.xyz/readme/cysic-agent-to-agent-protocol\",\n",
    "    \"https://docs.cysic.xyz/\"\n",
    "]\n",
    "url_loader = WebBaseLoader(base_urls)\n",
    "url_docs = url_loader.load()\n",
    "url_chunks = text_splitter.split_documents(url_docs)\n",
    "\n",
    "url_store = Chroma.from_documents(\n",
    "    documents=url_chunks,\n",
    "    collection_name=\"url_docs\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_store_url\"\n",
    ")\n",
    "url_retriever = url_store.as_retriever(search_kwargs={\"k\": 5})  # Limit results\n",
    "\n",
    "# --- Tweet Retriever (Enhanced filtering for meaningful content) ---\n",
    "def filter_meaningful_tweets(tweets):\n",
    "    \"\"\"Filter and process tweets to store only meaningful, generic content\"\"\"\n",
    "    if not tweets:  # Handle empty list\n",
    "        return []\n",
    "        \n",
    "    meaningful_tweets = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # Ensure tweet is a string\n",
    "        if not isinstance(tweet, str):\n",
    "            continue\n",
    "            \n",
    "        # Skip if too short (less than 10 characters to be more lenient)\n",
    "        if len(tweet.strip()) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Skip if contains too many mentions or hashtags (spam-like)\n",
    "        mention_count = tweet.count('@')\n",
    "        hashtag_count = tweet.count('#')\n",
    "        if mention_count > 5 or hashtag_count > 8:  # More lenient\n",
    "            continue\n",
    "            \n",
    "        # Skip promotional/spam content\n",
    "        spam_keywords = ['buy now', 'limited time', 'click here', 'dm me', 'follow for follow', 'get rich quick']\n",
    "        if any(keyword in tweet.lower() for keyword in spam_keywords):\n",
    "            continue\n",
    "            \n",
    "        # Keep educational, informational, or engaging content\n",
    "        meaningful_keywords = ['learn', 'understand', 'explore', 'discover', 'innovation', \n",
    "                             'technology', 'development', 'community', 'future', 'breakthrough',\n",
    "                             'cysic', 'blockchain', 'crypto', 'defi', 'web3', 'protocol']\n",
    "        \n",
    "        # More lenient criteria: keep if has meaningful keywords OR is reasonably long OR mentions project\n",
    "        if (any(keyword in tweet.lower() for keyword in meaningful_keywords) or \n",
    "            len(tweet) > 80 or \n",
    "            'cysic' in tweet.lower()):\n",
    "            meaningful_tweets.append(tweet.strip())\n",
    "    \n",
    "    return meaningful_tweets\n",
    "\n",
    "# Process Tweepy response object properly\n",
    "def extract_tweet_texts(tweets_response):\n",
    "    \"\"\"Extract tweet texts from Tweepy response object\"\"\"\n",
    "    if not tweets_response or not hasattr(tweets_response, 'data') or not tweets_response.data:\n",
    "        return []\n",
    "    \n",
    "    tweet_texts = []\n",
    "    for tweet in tweets_response.data:\n",
    "        if hasattr(tweet, 'text'):\n",
    "            tweet_texts.append(tweet.text)\n",
    "    \n",
    "    return tweet_texts\n",
    "\n",
    "# Check if 'tweets' variable exists and handle appropriately\n",
    "try:\n",
    "    if 'tweets' in locals() or 'tweets' in globals():\n",
    "        # Extract text from Tweepy response\n",
    "        raw_tweet_texts = extract_tweet_texts(tweets)\n",
    "        filtered_tweets = filter_meaningful_tweets(raw_tweet_texts)\n",
    "        print(f\"Original tweets: {len(raw_tweet_texts)}\")\n",
    "        print(f\"Filtered tweets: {len(filtered_tweets)}\")\n",
    "        \n",
    "        # Print sample of filtered tweets for debugging\n",
    "        if filtered_tweets:\n",
    "            print(\"\\nSample filtered tweets:\")\n",
    "            for i, tweet in enumerate(filtered_tweets[:3], 1):\n",
    "                print(f\"{i}. {tweet[:100]}...\")\n",
    "                \n",
    "    elif 'tweet' in locals() or 'tweet' in globals():\n",
    "        # Handle if you named it 'tweet' instead of 'tweets'\n",
    "        if hasattr(tweet, 'data'):\n",
    "            raw_tweet_texts = extract_tweet_texts(tweet)\n",
    "            filtered_tweets = filter_meaningful_tweets(raw_tweet_texts)\n",
    "        else:\n",
    "            filtered_tweets = filter_meaningful_tweets(tweet)\n",
    "        print(f\"Filtered tweets: {len(filtered_tweets)}\")\n",
    "        \n",
    "    else:\n",
    "        # If no tweets available, create some sample meaningful tweets for testing\n",
    "        print(\"No 'tweets' variable found. Using sample tweets for testing.\")\n",
    "        sample_tweets = [\n",
    "            \"Exploring the future of decentralized protocols with Cysic's innovative approach to blockchain technology.\",\n",
    "            \"The community is growing! Excited to see more developers joining the Cysic ecosystem.\",\n",
    "            \"Understanding zero-knowledge proofs and their role in modern cryptocurrency systems.\",\n",
    "            \"Innovation in blockchain requires both technical excellence and community collaboration.\",\n",
    "            \"Building the future of Web3 infrastructure one protocol at a time.\"\n",
    "        ]\n",
    "        filtered_tweets = sample_tweets\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing tweets: {e}\")\n",
    "    # Fallback to sample tweets\n",
    "    filtered_tweets = [\n",
    "        \"Cysic is revolutionizing blockchain technology through innovative protocols.\",\n",
    "        \"Join our growing community of developers building the future of DeFi.\",\n",
    "        \"Learn about the latest developments in zero-knowledge proof systems.\"\n",
    "    ]\n",
    "\n",
    "# Only create tweet store if we have tweets\n",
    "if filtered_tweets:\n",
    "    tweet_docs = [Document(page_content=t, metadata={\"source\": \"twitter\", \"type\": \"meaningful_content\"}) \n",
    "                  for t in filtered_tweets]\n",
    "\n",
    "    tweet_store = Chroma.from_documents(\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"tweet_docs\",\n",
    "        documents=tweet_docs,\n",
    "        persist_directory=\"./chroma_store_tweet\"\n",
    "    )\n",
    "    tweet_retriever = tweet_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "else:\n",
    "    print(\"Warning: No meaningful tweets found. Tweet retriever will return empty results.\")\n",
    "    # Create a dummy retriever that returns empty results\n",
    "    class EmptyRetriever:\n",
    "        def get_relevant_documents(self, query):\n",
    "            return []\n",
    "    \n",
    "    tweet_retriever = EmptyRetriever()\n",
    "\n",
    "# -------------------------\n",
    "# TOOLS (Enhanced with better content processing)\n",
    "# -------------------------\n",
    "MAX_OUTPUT_CHARS = 4000  # Reduced for better LLM processing\n",
    "\n",
    "@tool\n",
    "def text_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant content from local TXT (Cysic whitepaper) and extract key themes.\"\"\"\n",
    "    results = txt_retriever.get_relevant_documents(query)\n",
    "    if not results:\n",
    "        return \"No relevant content found in whitepaper.\"\n",
    "    \n",
    "    content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "    return f\"WHITEPAPER CONTENT:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "\n",
    "@tool\n",
    "def url_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant content from Cysic documentation URLs.\"\"\"\n",
    "    results = url_retriever.get_relevant_documents(query)\n",
    "    if not results:\n",
    "        return \"No relevant content found in documentation.\"\n",
    "    \n",
    "    content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "    return f\"DOCUMENTATION CONTENT:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "\n",
    "@tool\n",
    "def tweet_loader(query: str) -> str:\n",
    "    \"\"\"Retrieve meaningful tweet examples and recent updates.\"\"\"\n",
    "    try:\n",
    "        results = tweet_retriever.get_relevant_documents(query)\n",
    "        if not results:\n",
    "            return \"No relevant tweets found. Focus on creating original content based on whitepaper and documentation.\"\n",
    "        \n",
    "        content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "        return f\"RECENT MEANINGFUL TWEETS:\\n{content[:MAX_OUTPUT_CHARS]}\"\n",
    "    except Exception as e:\n",
    "        return \"Tweet retriever not available. Focus on creating original content from other sources.\"\n",
    "\n",
    "@tool\n",
    "def content_synthesizer(retrieved_content: str) -> str:\n",
    "    \"\"\"Synthesize retrieved content into Kaito-worthy tweet themes and key points.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following retrieved content and extract 5-7 compelling tweet themes that meet Kaito guidelines:\n",
    "\n",
    "    {retrieved_content}\n",
    "\n",
    "    For each theme, provide:\n",
    "    1. A thought-provoking hook or insight\n",
    "    2. Technical depth that shows expertise\n",
    "    3. Discussion-worthy angles that invite engagement\n",
    "    4. Clear value proposition for the crypto community\n",
    "\n",
    "    Focus on Kaito-approved content types:\n",
    "    - Deep technical insights and explanations\n",
    "    - Market analysis and ecosystem developments  \n",
    "    - Educational content that teaches something new\n",
    "    - Contrarian takes backed by solid reasoning\n",
    "    - Future predictions with logical foundations\n",
    "    - Real-world use cases and adoption stories\n",
    "\n",
    "    ❌ AVOID Kaito red flags:\n",
    "    - Pure price speculation without substance\n",
    "    - Overly promotional language\n",
    "    - Recycled generic content\n",
    "    - Unsubstantiated claims\n",
    "    - Low-effort commentary\n",
    "\n",
    "    Format as actionable tweet concepts that would earn high Kaito scores.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "@tool\n",
    "def tweet_creator(content: str) -> str:\n",
    "    \"\"\"Create Kaito-compliant tweets that drive meaningful engagement.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following information, create compelling Twitter content optimized for Kaito guidelines:\n",
    "\n",
    "    {content}\n",
    "\n",
    "    Create tweets that meet Kaito's high standards:\n",
    "\n",
    "    ✅ KAITO-APPROVED ELEMENTS:\n",
    "    - Lead with counterintuitive insights or fresh perspectives\n",
    "    - Include specific technical details or data points\n",
    "    - Pose thought-provoking questions that spark debate\n",
    "    - Share actionable insights the audience can use\n",
    "    - Reference concrete examples or real developments\n",
    "    - Build logical arguments with clear reasoning chains\n",
    "    - Address common misconceptions with facts\n",
    "\n",
    "    📊 ENGAGEMENT OPTIMIZATION:\n",
    "    - Start with a bold statement or surprising fact\n",
    "    - Use \"Here's why...\" or \"Most people miss...\" hooks\n",
    "    - Include 1-2 strategic questions to drive replies\n",
    "    - Add context that helps readers understand significance\n",
    "    - End with clear takeaways or implications\n",
    "\n",
    "    🎯 STRUCTURE:\n",
    "    - Tweet 1: Hook + core insight + context\n",
    "    - Tweet 2 (if needed): Supporting evidence + implications  \n",
    "    - Tweet 3 (if needed): Question/CTA + broader significance\n",
    "\n",
    "    ❌ AVOID:\n",
    "    - Generic motivational content\n",
    "    - Pure price/token speculation\n",
    "    - Obvious statements without depth\n",
    "    - Clickbait without substance\n",
    "    - Overly technical jargon without explanation\n",
    "\n",
    "    🔥 KAITO SUCCESS FORMULA:\n",
    "    Insight + Evidence + Implication + Engagement Hook\n",
    "\n",
    "    Return 1-3 tweets formatted and ready to post, optimized for maximum Kaito score and community engagement.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "@tool\n",
    "def quality_reviewer(tweet_content: str) -> str:\n",
    "    \"\"\"Review tweets against Kaito guidelines and engagement potential.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Review this tweet content against Kaito's quality standards and engagement optimization:\n",
    "\n",
    "    {tweet_content}\n",
    "\n",
    "    Evaluate and improve based on Kaito criteria:\n",
    "\n",
    "    📈 KAITO SCORING FACTORS:\n",
    "    1. Educational Value: Does it teach something new?\n",
    "    2. Technical Depth: Shows genuine expertise?  \n",
    "    3. Engagement Potential: Likely to spark meaningful discussion?\n",
    "    4. Originality: Fresh perspective or unique insight?\n",
    "    5. Community Value: Useful for the crypto community?\n",
    "    6. Reasoning Quality: Logical and well-supported?\n",
    "\n",
    "    💬 ENGAGEMENT OPTIMIZATION:\n",
    "    - Does it invite thoughtful responses (not just likes)?\n",
    "    - Will people want to share/quote tweet with their thoughts?\n",
    "    - Does it position followers as early/informed?\n",
    "    - Are there clear discussion hooks?\n",
    "\n",
    "    🎯 IMPROVEMENT AREAS:\n",
    "    - Strengthen the core insight if too generic\n",
    "    - Add specific examples or data points\n",
    "    - Include contrarian angles that challenge assumptions\n",
    "    - Enhance the \"so what?\" factor\n",
    "    - Improve question formulation for better replies\n",
    "    - Ensure technical accuracy and clarity\n",
    "\n",
    "    Return the enhanced version that maximizes both Kaito score and viral engagement potential.\n",
    "    Make it content people will want to \"yap\" about and discuss extensively.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "@tool\n",
    "def kaito_compliance_checker(content: str) -> str:\n",
    "    \"\"\"Final check against Kaito guidelines for maximum engagement and platform approval.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Perform final Kaito compliance check and optimization:\n",
    "\n",
    "    {content}\n",
    "\n",
    "    KAITO PLATFORM REQUIREMENTS:\n",
    "    \n",
    "    🎯 CONTENT QUALITY CHECKLIST:\n",
    "    ✅ Educational/Informational value (not just opinion)\n",
    "    ✅ Technical depth appropriate for crypto audience  \n",
    "    ✅ Original insights (not recycled content)\n",
    "    ✅ Factual accuracy and verifiable claims\n",
    "    ✅ Clear logical reasoning chain\n",
    "    ✅ Actionable takeaways for readers\n",
    "\n",
    "    💬 ENGAGEMENT OPTIMIZATION:\n",
    "    ✅ Thought-provoking questions or contrarian takes\n",
    "    ✅ Discussion hooks that invite meaningful replies\n",
    "    ✅ Community-relevant insights \n",
    "    ✅ Positions readers as informed/early\n",
    "    ✅ Shareable with added commentary potential\n",
    "\n",
    "    📊 KAITO SUCCESS METRICS:\n",
    "    - High reply-to-like ratio (discussion over passive engagement)\n",
    "    - Quote tweets with thoughtful additions\n",
    "    - Bookmarks for reference value\n",
    "    - Profile visits from curious readers\n",
    "    - Follow-up thread requests\n",
    "\n",
    "    🚫 KAITO RED FLAGS TO AVOID:\n",
    "    - Pure price speculation without analysis\n",
    "    - Generic motivational content\n",
    "    - Unsubstantiated hype or FUD\n",
    "    - Content that doesn't add new value\n",
    "    - Overly promotional language\n",
    "\n",
    "    Return the final optimized version that maximizes Kaito approval and creates \"yap-worthy\" content that people will actively discuss and debate.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "@tool  \n",
    "def final_formatter(content: str) -> str:\n",
    "    \"\"\"Format the final Kaito-optimized tweet for publishing.\"\"\"\n",
    "    return {\n",
    "        \"final_tweet\": content.strip(),\n",
    "        \"kaito_optimized\": True,\n",
    "        \"engagement_type\": \"discussion_driven\"\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# AGENTS (Enhanced with specific roles)\n",
    "# -------------------------\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[text_loader, url_loader, tweet_loader],\n",
    "    name=\"research_agent\",\n",
    "    prompt=\"\"\"\n",
    "    You are a research specialist focused on gathering comprehensive information about Cysic.\n",
    "    \n",
    "    For every query:\n",
    "    1. ALWAYS call text_loader first to get whitepaper insights\n",
    "    2. ALWAYS call url_loader to get documentation content  \n",
    "    3. ALWAYS call tweet_loader to understand recent community discussions\n",
    "    \n",
    "    Your goal is to gather diverse, rich information that can be used to create engaging tweets.\n",
    "    Collect information about technology, community updates, innovations, and user benefits.\n",
    "    \n",
    "    After calling all three tools, summarize the key findings and interesting angles discovered.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "synthesis_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[content_synthesizer],\n",
    "    name=\"synthesis_agent\", \n",
    "    prompt=\"\"\"\n",
    "    You are a content strategist who transforms raw information into compelling tweet concepts.\n",
    "    \n",
    "    Take the research findings and use content_synthesizer to:\n",
    "    - Identify the most interesting and engaging angles\n",
    "    - Create multiple tweet theme options\n",
    "    - Focus on what would genuinely interest Twitter users\n",
    "    - Highlight unique insights and valuable information\n",
    "    - Consider current trends in crypto and tech Twitter\n",
    "    \n",
    "    Always call content_synthesizer exactly once with all the research data.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "creative_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[tweet_creator],\n",
    "    name=\"creative_agent\",\n",
    "    prompt=\"\"\"\n",
    "    You are a creative Twitter expert who crafts viral-worthy content.\n",
    "    \n",
    "    Using the synthesized themes and the user's specific request:\n",
    "    1. Call tweet_creator to generate fresh, engaging tweets\n",
    "    2. Focus on creating content that stops the scroll\n",
    "    3. Balance information with entertainment value\n",
    "    4. Ensure each tweet provides clear value to readers\n",
    "    5. Use hooks, storytelling, and engagement tactics\n",
    "    \n",
    "    Always prioritize freshness and originality over recycling existing content.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "quality_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[quality_reviewer],\n",
    "    name=\"quality_agent\",\n",
    "    prompt=\"\"\"\n",
    "    You are a social media quality assurance expert.\n",
    "    \n",
    "    Review the created tweet content using quality_reviewer to ensure:\n",
    "    - Maximum engagement potential\n",
    "    - Clear value proposition for readers  \n",
    "    - Strong attention-grabbing elements\n",
    "    - Professional yet conversational tone\n",
    "    - Optimal structure and flow\n",
    "    \n",
    "    Polish the content to Twitter best practices standards.\n",
    "    Call quality_reviewer exactly once.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "publishing_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[final_formatter],\n",
    "    name=\"publishing_agent\",\n",
    "    prompt=\"\"\"\n",
    "    You are the final publishing coordinator.\n",
    "    \n",
    "    Take the quality-reviewed content and use final_formatter to prepare it for publication.\n",
    "    Ensure the tweet is properly formatted and ready to post.\n",
    "    \n",
    "    Call final_formatter exactly once with the polished content.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# SIMPLIFIED WORKFLOW (Single Graph Approach)\n",
    "# -------------------------\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def research_node(state):\n",
    "    \"\"\"Research phase: gather all information\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    query = messages[-1].content\n",
    "    \n",
    "    # Gather information from all sources\n",
    "    txt_content = text_loader.invoke({\"query\": query})\n",
    "    url_content = url_loader.invoke({\"query\": query})\n",
    "    tweet_content = tweet_loader.invoke({\"query\": query})\n",
    "    \n",
    "    research_summary = f\"\"\"\n",
    "    RESEARCH FINDINGS FOR: {query}\n",
    "    \n",
    "    {txt_content}\n",
    "    \n",
    "    {url_content}\n",
    "    \n",
    "    {tweet_content}\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=research_summary)]}\n",
    "\n",
    "def synthesis_node(state):\n",
    "    \"\"\"Synthesis phase: extract themes\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    research_data = messages[-1].content\n",
    "    \n",
    "    themes = content_synthesizer.invoke({\"retrieved_content\": research_data})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=themes.content)]}\n",
    "\n",
    "\n",
    "def creation_node(state):\n",
    "    \"\"\"Creation phase: generate actual tweets from synthesized themes\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    themes_content = messages[-1].content\n",
    "    \n",
    "    # Extract the user's original query to maintain context\n",
    "    original_query = messages[0].content\n",
    "    \n",
    "    # Create the prompt for tweet creation\n",
    "    creation_input = f\"\"\"\n",
    "    ORIGINAL REQUEST: {original_query}\n",
    "    \n",
    "    SYNTHESIZED THEMES AND INSIGHTS:\n",
    "    {themes_content}\n",
    "    \"\"\"\n",
    "    \n",
    "    created_tweets = tweet_creator.invoke({\"content\": creation_input})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=created_tweets.content)]}\n",
    "\n",
    "def quality_node(state):\n",
    "    \"\"\"Quality phase: review and improve\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    draft_tweet = messages[-1].content\n",
    "    \n",
    "    polished_tweet = quality_reviewer.invoke({\"tweet_content\": draft_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=polished_tweet.content)]}\n",
    "\n",
    "def kaito_check_node(state):\n",
    "    \"\"\"Kaito compliance check: ensure maximum platform approval\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    polished_tweet = messages[-1].content\n",
    "    \n",
    "    kaito_optimized = kaito_compliance_checker.invoke({\"content\": polished_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=kaito_optimized.content)]}\n",
    "\n",
    "def final_node(state):\n",
    "    \"\"\"Final phase: format Kaito-optimized tweet for publication\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    kaito_tweet = messages[-1].content\n",
    "    \n",
    "    final_result = final_formatter.invoke({\"content\": kaito_tweet})\n",
    "    \n",
    "    return {\"messages\": messages + [AIMessage(content=final_result[\"final_tweet\"])]}\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.add_node(\"synthesis\", synthesis_node) \n",
    "workflow.add_node(\"creation\", creation_node)\n",
    "workflow.add_node(\"quality\", quality_node)\n",
    "workflow.add_node(\"kaito_check\", kaito_check_node)  # New Kaito compliance node\n",
    "workflow.add_node(\"final\", final_node)\n",
    "\n",
    "# Define the enhanced flow\n",
    "workflow.set_entry_point(\"research\")\n",
    "workflow.add_edge(\"research\", \"synthesis\")\n",
    "workflow.add_edge(\"synthesis\", \"creation\")\n",
    "workflow.add_edge(\"creation\", \"quality\")\n",
    "workflow.add_edge(\"quality\", \"kaito_check\")  # Add Kaito check step\n",
    "workflow.add_edge(\"kaito_check\", \"final\")\n",
    "\n",
    "# Compile the application\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your input should be a list of messages, not a single message\n",
    "input_messages = [HumanMessage(content=\" Could ZKPs revolutionize not just transactions but also data protection in a quantum computing era? \")]  # Note the brackets\n",
    "\n",
    "# Better execution approach\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_query_id\": \"1\", \"recursion_limit\": 1000}}\n",
    "\n",
    "# Get just the final result\n",
    "final_result = app.invoke({\"messages\": input_messages}, config)\n",
    "final_tweet = final_result[\"messages\"][-1].content\n",
    "\n",
    "print(\"🎯 FINAL CYSIC TWEET:\")\n",
    "print(\"=\"*50)\n",
    "print(final_tweet)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1158065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb59c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
